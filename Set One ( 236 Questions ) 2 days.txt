Set One
======

1) About different distributions of Hadoop’s?
Answer: Cloudera, HortonWorks, MapR, Apache.

2) Which is the best operating system to run Hadoop?
Answer:
Ubuntu or Linux is the most preferred operating system to run Hadoop. 

3) What are the daemons required to run a Hadoop cluster?
Answer:
Name Node, Data Node, Task Tracker and Job Tracker

4)  Is it the data stored on RDBMS or any database?
Answer:
Ans:- structured data is stored on RDBMSs and log files are stored on hdfs.

5) What are different deamons for hadoop?
Answer:
ans:- NameNode, SecondaryNameNode, Data Node, Resource Manager and Node Manager.

6) What are the services in hadoop ecosystem?
Answer:
Ans;- pig, sqoop, hive, hue, flume, kerbros, zooKeeper, oozie, hdfs

7) Hadoop 1 or Hadoop 2 which one you are using.
Answer:
Hadoop 2.x is preferred because of YARN. Resource allocation, High Availability and Federation

8) What is problem solved to hadoop one?
Answer:
A. We can storage and batch processing. you can do itl.

9) How will you actually configure your variuables of enviroments?
Answer:
A. Bachrc configure

10) How  does writes and reads hadoop data?
Answer:
A. Hadoop writes data in pipeline. hadoop reads data in parallel.

11) For hadoop 2 installation what are prerequisites?
Answer:
1) Take controller server
2) Update to repository
3) Create user
4) Create enviroment variables
5) Disable seliunx
6) Disable ipv6
7) Disable firewall iptables
8) Diable transparent hugepage compaction
9) Set swappiness
10) Configure ntp
11) Configure ssh passwaord login

12) Why three replication in namenode?
Answer:
According to quality clod, hot and warm.

13) What is hadoop file system?
Answer:
A. fsimage and edits
RAM- edits
Hard Disk- fsimage

14) How many copies meta data do you have?
Answer:
A. minimun is three and best is six

15) What is safe mode in hadoop?
Answer:
A. Read only mode

16) What does 'jps' command do?
Ans. 
- It gives the status of the deamons which run Hadoop cluster. 

17) How to restart Namenode?
Ans. 
stop-all.sh and then start-all.sh 

18) What are the three V’s commonly used to describe Big Data?
Volume, Velocity & Variety.

19) What are the three Big Data formats?
Structure, Semi-structured & Un-structure

20) List one example of structured data.
Traditional database

21) Do you follow a standard procedure to deploy Hadoop?
Ans. Yes we refer the Hortonworks Documentation.

22) Which file contains the property that configures the HDFS superuser?
hdfs-site.xml

23) List out Hadoop’s configuration files?
The three configuration files are
•	core-site.xml
•	mapred-site.xml
•	hdfs-site.xml

24) What is hive port number?
Ans: 10000

25) which connector for hive
Ans: ODBC/JDBC

26) Where the metadata information get store?
Answer:	It gets stored in the NameNode Memory.

27) How many Name Nodes can you run on a single Hadoop cluster?
Only one

28) Do you have the root permission for your cluster.
Answer:
To deploy cloudera we need root permission, we have to approach the client in order to get the root permission.

29) What is commodity hardware? 
Answer:
Commodity Hardware refers to inexpensive systems that do not have high availability or high quality.

30) Can i access Hive Without Hadoop ?
Answer:
Yes,We can access Hive without hadoop with the help of other data storage systems like Amazon S3, GPFS (IBM) and MapR file system.

31) Have you set the preemptions? *
Ans:
Yes,we set it to get more performance and well job scheduling.

32) Which are the three modes in which Hadoop can be run?
Ans. - 
1. standalone (local) mode 
2. Pseudo-distributed mode 
3. Fully distributed mode

33) What if a Namenode has no data?
Ans. 
- It cannot be part of the Hadoop cluster. Becuase namenode store only metadata. actual data store in datanode.

34) What happens to job tracker when Namenode is down?
Ans. - When Namenode is down, your cluster is OFF, this is because Namenode is the single point of failure in HDFS.

35) If A Data Node Is Full How It's Identified?
Answer :
When data is stored in datanode, then the metadata of that data will be stored in the Namenode. So Namenode will identify if the data node is full.

36) Why 'reading' Is Done In Parallel And 'writing' Is Not In Hdfs?
Answer :
The reason is that if we perform the write operation in parallel, then it might result in data inconsistency. 

37) What are different hdfs dfs shell commands to perform copy operation?
$ hadoop fs -copyToLocal
$ hadoop fs -copyFromLocal
$ hadoop fs -put

38) What is HDFS block size and what did you chose in your project?
By default, the HDFS block size is 64MB. It can be set to higher values as 128MB or 256MB. 128MB is acceptable industry standard.

39)  What is failover controllers?
Answer:	
Failover: It is to automatically start the standby Namenode in case of the NameNode failure. 

40) How can you increase the Name Node heap memory
We can increase the name node heap memory in the hadoop-env.sh file in /etc/hadoop/conf folder by changing the XmX value of the property: “HADOOP.NAMENODE_OPTS”

41) Explain what is difference between an Input Split and HDFS Block?
Logical division of data is known as Split while physical division of data is known as HDFS Block

42) In Hadoop what is InputSplit?
It splits input files into chunks and assign each split to a mapper for processing.

43) What is Journal Node?
Standby Nodes and Active Nodes communicate with a group of light weight nodes to keep their state synchronized. These are known as Journal Nodes.

44) Where the encryption keys are stored?
Encryption keys should be stored on separate machines from the data they are used to unlock and also on KMS (Key management server)

45) What Is The Use Of The Command Mapred.job.tracker?
Answer :
It is used by the Job Tracker to list out which host and port that the MapReduce job tracker runs at.

46) How Can We Look For The Namenode In The Browser?
Answer :
The port number to look for Namenode in the browser is 50070.

47) What are the hadoop stack in your cluster
Ans:
HDFS, MapReduce, Yarn, Hive, Pig, Flume, Hue, Sqoop, Zookeeper, and CMS.

48) Can We Have Multiple Entries In The Master Files?
Answer :
Yes, we can have multiple entries in the Master files.

49) Can You Tell Me If We Can Create A Hadoop Cluster From Scratch?
Answer :
Yes, we can definitely do that.  Once we become familiar with the Apache Hadoop environment, we can create a cluster from scratch.

50) Is It Necessary That Name Node And Job Tracker Should Be On The Same Host?
Answer :
No! They can be on different hosts.

51) Which One Decides The Input Split - Hdfs Client Or Namenode?
Answer :
The HDFS Client does not decide. It is already specified in one of the configurations through which input split is already configured.

52) Mention what is the number of default partitioner in Hadoop?
In Hadoop, the default partitioner is a “Hash” Partitioner.

53) Mention how Hadoop is different from other data processing tools?
In Hadoop, you can increase or decrease the number of mappers without worrying about the volume of data to be processed.

54) Mention what job does the conf class do?
It is separate different jobs running on the same cluster.  It does the job level settings such as declaring a job in a real environment.

55) What is preemption?
Ans:
Preempting an application means containers from other applications may need to be killed if necessary, to make room for the new applications.

56) What happens when the Name Node on the Hadoop cluster goes down?
The file system goes offline whenever the Name Node is down.

57) I have job of 99 mapers and 1 reducer how do you optimize the performance?
Ans: 
we can add combiner as a mini reducer which can compress the o/p of mapper and send it to the reducer.

58) Mention what is distributed cache in Hadoop?
It is a facility provided by MapReduce framework. At the time of execution of the job, it is used to cache file. 

59) Mention how many InputSplits is made by a Hadoop Framework?
Hadoop will make 5 splits
•	1 split for 64K files
•	2 split for 65mb files
•	2 splits for 127mb files

60) What is zkfc.?
Answer:
The ZooKeeper Failover Controller (ZKFC) is responsible for HA Monitoring of the NameNode service and for automatic failover when the Active NameNode is unavailable.

61) Can i kill the specific process?

Answer:
Yes, using command 
kill -9 1234 (pid)

62) Do you HA of AD?

Answer:
No, we don't have ha for ad in our cluster. we mostly perfer ha for namenode, yarn and hive.

63) Can we create keytab with kadmin.local command?

Answer: 
Yes

64) What is the command to check cup utilization?

Anser: 
The iostat command reports Central Processing Unit (CPU) statistics and input/output statistics for devices and partitions.

65) How can i list the zombie process from CLI?

Answer: 
ps aux | grep 'Z'

66) What Is A Task Tracker In Hadoop? How Many Instances Of Tasktracker Run On A Hadoop Cluster?
Answer :
A TaskTracker is a slave node daemon in the cluster that accepts tasks from a JobTracker. 
There is only One Task Tracker process run on any hadoop slave node. 

67) How does a Name Node confirm that a particular node is dead?
Name node is receive heartbeat signal from the data nodes in every 3 seconds, if not received until 10 min then the name node will declare that data node to be dead.

68) What is distcp?
Answer:
1. distcp is the program comes with Hadoop for copying large amount of data to and from Hadoop file systems in parallel

69) What is the main purpose of HDFS fsck command?
Answer:
fsck a utility to check health of the file system, to find missing files, over-replicated, under-replicated and corrupted blocks.

70) Explain what are the basic parameters of a Mapper?
Answer:
I)  LongWritable and Text  
II) Text and IntWritable

71) Can Hadoop handle streaming data?
Ans. 
- Yes, through Technologies like Apache Kafka, Apache Flume, and Apache Spark. it is possible to do large-scale streaming.

72) What Is Hdfs?
Answer :
HDFS is filing system use to store large data files. It handles streaming data and running clusters on the commodity hardware.

73) How Many Instances Of Tasktracker Run On A Hadoop Cluster? (repeat)
Answer :
There is one Daemon Tasktracker process for each slave node in the Hadoop cluster.

74) How Many Daemon Processes Run On A Hadoop Cluster?
Answer :
Hadoop is comprised of five separate daemons. Each of these daemons runs in its own JVM.

75) Which framework provides a high-performance coordination service for distributed applications?
Zookeeper

76) Will DRF dominantley take allocated cpu or memory or both from other que?
Ans:
Yes ,it is take allocated cpu or ram and either both from the queue as per weightage given to the pool.

77) What will be the size of fsimage?
Answer:	That depends on the size of number of blocks and directories contained in the cluster.

78) Which command is used to do a file system check in HDFS?
hadoop fsck

79) What is the relationship between data node & node manager ?
The Node Manager is the YARN worker component while a Data Node is an HDFS worker component

80) Have you ever configured Capacity Scheduler?
Yes I’ve done that, Using Ambari Views (YARN Queue Manager)

81) How can you decide the heap memory limit for a Name Node and Hadoop Service?
Name Node heap size depends on many factors such as the number of files, the number of blocks, and the load on the system.

82) Is your KDC server in AD, do you have control over AD?

Ans: Yes, but we don't have control

83) How many types of scheduler
 and Which scheduler you are using?

Answer: 
Fair Scheduler & Capacity scheduler. We are using Fair Scheduler

84) What are you using for hive-shell
?
Answer:
we are using beeline promt

85) How many option do we have for RDBMS DB 
which ones have you worked on?

Answer:
a) mySQL (default) b) Oracle c) Postgres

86) Do you have mySQL database's HA?

Answer:
No, we are not perfer my SQL we are mostly use for Yarn, Hdfs HA.

87) Can i manage the cluster without zookeeper services
?
Answer: 
Yes, but there will be lot of issue like synchronisation issue, cordinated data n many more

88) Do you know about spark have you worked on it?

Answer:
Yes, I know about the spark but in our cluster we are not using spark. 

89) List three reasons to add or remove cluster nodes.
Hardware Failure, Software Upgrade, Hardware Obsolete

90) Mention what is the data storage component used by Hadoop?
 HBase.

91) Which command is used to verify if the HDFS is corrupt or not?
 Hadoop FSCK (File System Check) command is used to check missing blocks.

92) When configuring Hadoop manually, which property file should be modified to configure slots?
In Hadoop 1 its mapred-site.xml & in Hadoop 2 its yarn-site.xml

93) What was the unstructed data that was received.
log files, images, movies etc. 

94) What is the name of the configuration file for configuring capacity scheduler?
Ans.  (Hadoop 1) mapred-site.xml & (Hadoop 2) yarn-site.xml

95) Which  file u will refer to perform clusterwide operation?
Answer: core-site.xml

96) Which of the three Hadoop deployment modes is commonly used for production environments
Distributed Mode

97) Heap memory is responsible for which component ?
Ans. Heap memory is responsible for all the components in Hadoop as all of them run on JVM.

98) - What is the communication channel between client and namenode/datanode?
Ans. - The mode of communication is SSH.

99) - Copy a directory from one node in the cluster to another
Ans. - Use '-distcp' command to copy,

100) - Default replication factor to a file is 3.Use '-setrep' command to change replication factor of a file to 2.
Ans. - hadoop fs -setrep -w 2 apache_hadoop/sample.txt

101) - Is there a hdfs command to see available free space in hdfs
Ans. 
- hadoop dfsadmin -report

102) What will happen if target-dir already exists during sqoop import?
Answer:
- Sqoop runs a map-only job and if target directory already exists, it will throw an execption.

103) What is the default number of mapper in a sqoop job?
Answer: 5

104) The DistCp –update option ensures that only files that meet two criteria are copied. What are those criteria?
Answer:
The file does not exist at the destination, or the file at the destination has different contents.

105) Which DistCp mode might typically be faster, static or dynamic?
Answer: Dynamic. 

106) List the two types of ACL masks.
Answer: Access masks and default masks

107) What is the advantage of turning on maintenance mode when decommissioning or deleting worker nodes?
Answer:
Turning on Maintenance mode will suppress alerts, warnings and status change indicator generated for the object

108) When Name Node is down, what does the Job Tracker do? 
When Name node is down the entire cluster is inaccessible, hence the job tracker will also be down.

109) Which type of Hadoop cluster nodes provide resources for data processing?
Slaves Nodes

110) Mention Hadoop core components?
•	HDFS
•	MapReduce

111) Which tool will you prefer to use for monitoring Hadoop and HBase clusters?
Ans. Ambari Metrics

112) In my company we don't have ipv4 we have ipv6 only hadoop can not work here?
A. ipv6 only support in hadoop 3 version. ipv6 is not support in hadoop 2.

113) Have you worked on vanila, hbase, kafka, ansible hadoop?

Answer: 
No i havent got the opportunity to do it but i am ready to learn if i get an opportunity.

114) What happen to existing data if we change block size in HDFS?
Answer:
If you change the block size in hdfs-site.xml configuration file, it won't affect the existing data which is already stored in HDFS

115) cm ports?
ANSWER:
Cloudera Manager Server 	7180/7183
Cloudera Manager Agent            9000
Cloudera Manager Event Server 7184

116) Which service manages cluster storage resources?
HDFS

117) Which HDFS Shell command displays the number of bytes in a file or directory?
The du command

118) How does a Name Node determine Data Node availability?
The Name Node listens for heartbeats from the Data Nodes. Heartbeats arrive every three seconds by default.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Set Two
======

119) On a Name Node, what type of information is contained within an fsimage file?
The fsimage file contains a static image of the state of the HDFS file system at a specific point in time

120) If HDFS data is already redundant across multiple Data Nodes, why do you need to back it up?
Because disasters happen that can affect more than one system or an entire data center.

121) How are HDFS snapshots useful for disaster recovery?
Because snapshots can be replicated to another cluster that could take over responsibility for a failed cluster.

122) What does the HDFS balancer do?
The HDFS balancer moves data blocks from over-utilized Data Nodes to under-utilized Data Nodes.

123) Which command is used to verify if the HDFS is corrupt or not? (repeat)
 Hadoop FSCK (File System Check) command is used to check missing blocks

124) Is datanode logical or physical?
Answer:	A datanode is a physical device which reads and writes the data.

125) Where was your data?
Answer:	Most of our data was obtained from sensors. We used to keep the data in HDFS.

126) How to see underreplicated blocks in Hadoop?
Answer:	 Hadoop dfsadmin –reports

127) Does HDFS make block boundaries between records?
Yes

128) Command ktutil used for?
Answer:
The ktutil command invokes a command interface from which an administrator can read, write, or edit entries in a keytab.

129) Why is iptables disable?
A. Because hadoop is communicate in some ports which is above external.that's why disable iptables.

130) What two daemons / services make up the YARN framework?
Resource Manager / Node Manager

131) What construct is granted CPU and memory resources?
Container

132) What daemon / service are responsible for application fault tolerance?
Application Master

133) What daemon / service controls access to global cluster resources?
Resource Manager

134) What daemon / service make resource requests after an application has launched?
Application Master

135) What daemon / service manage file localization for applications?
Node Manager

136) The YARN component that enables multitenancy and adherence to Service Level Agreements is called 
The Resource Manager

137) List the YARN management options
Ambari Web UI, Resource Manager UI, Command line and manual configuration, and YARN API

138) Where can you quickly view the amount of utilized memory for the Node Managers in your cluster on a per-machine basis?
Service > YARN > Heat Maps

139) Which framework provides provisioning, management, and monitoring capabilities?
Ambari UI

140) Which Web interface provides detailed information on running applications?
Resourmanager Web UI

141) What CLI command will display a list of every node in the cluster regardless of its status?
Yarn nodes –list –all

142) What does file could only be replicated to 0 nodes, instead of 1 mean?
Ans. 
- The namenode does not have any available DataNodes.

143) What happens when a user submits a Hadoop job when the Name Node is down- does the job get in to hold or does it fail.
The Hadoop job fails when the Name Node is down.

144) What happens when a user submits a Hadoop job when the Job Tracker is down- does the job get in to hold or does it fail.
The Hadoop job fails when the Job Tracker is down.

145) The requirement is to add a new data node to a running Hadoop cluster; how do I start services on just one data node?
Ans. - You do not need to shutdown and/or restart the entire cluster in this case

146) Mention what is the best way to copy files between HDFS clusters?
The best way to copy files between HDFS clusters is by using multiple nodes and the distcp command, so the workload is shared.

147) Why yarn came into picture?
Scalability, Availability Issue, Problem with Resource Utilization, Limitation in running non-Map Reduce Application. These all issues in hadoop one that's why yarn came into picture

148) Suppose you disable selinux on hadoop? can i keep rdmbs hadoop now?
A. Hadoop is not compaliance with data compaliance. 
In that case we will disable selinux install hadoop then enable seliunx and put your data.

149) Assume you do not believe the information from the previous command is accurate. Which CLI command will update the node information at the Resource Manager?
Yarn rmadmin –refresh nodes

150) What are the ports used to access Resource Manager and Node Manager REST APIs?
Resource Manager 	8088 / Node Manager 8042

151) What component of the YARN stack is not directly monitored for failure by the Resource Manager?
Job tasks – these are monitored only by the Application Master.

152) By default, how long does the Resource Manager wait until concluding that a Node Manager has failed?
10 mins

153) By default, how frequently does the Node Manager send a heartbeat to the Resource Manager if it is functioning properly?
Every one second

154) Which YARN component is responsible for restarting a failed ApplicationMaster? ( TIll Yaad)
Resource Manager

155) Which HDP feature allows applications to continue rather than fail and relaunch after a Resource Manager failure?
Work Preserving Restarts

156) Which HDP feature centralizes YARN logging?
YARN log aggregation

157) Where is container located ?
Ans. Container is located in the Node Manager

158) Which service manages cluster CPU and memory resources?
Yarn

159) What exactly container contains?
Ans. It represents a collection of physical resources. Also could mean CPU cores, disk along with RAM

160) Where is container located ? (reapeat)
Ans. Container is located in the Node Manager.

161) What does resources means in Yarn?
Answer:	Resources in YARN means the Memory and the Cores.

162)  What does resources means in Yarn? (repeat)
Answer:	Resources in YARN means the Memory and the Cores.

163) How do you make a directory on hdfs ?
Ans: using hadoop fs -mkdir direcotry name.

164) How to copy the file from one cluster to another?
hadoop distcp hdfs://192.168.0.8:8020/input hdfs://192.168.0.9:8020/output

165) Where are Edit log stored ?
Dfs.namenode/version/current

166) Is datanode logical or physical? (repeat)
Answer:	A datanode is a physical device which reads and writes the data.

167) In Hadoop, which file controls reporting in Hadoop?
In Hadoop, the hadoop-metrics.properties file controls reporting.

168) How to configure trash?
Ans:-In core-site.xml we will set the trash interval and trash checkpoint interval.

169) Command to set replication factor?
Ans:-hdfs dfs -setrep

170) How do you copy a data frm local file to hdfs, tell me commands
ans:- hadoop fs -put source destination and file

171) What is commands to check which jobs are running?
Ans:- hadoop job -list.

172) Which version use on cdh?
Ans:- 5.8

173) How to do check physical memory using CLI?
Ans:- fsck

174) How do you check rep factor is ur data is replecated on not?
Ans:- admin report

175) Ratio between heap memory for mapper & reducer
Ans. The Ratio between heap memory for mapper & reducer is 2:1

176) Which MapReduce version have you configured on your Hadoop cluster?
Ans. Mapreduce version 2.0

177) What is Job Tracker?
Answer:	JobTracker is the service within Hadoop that runs MapReduce jobs on the cluster.

178) Which are the process required for YARN?
Answer:	In YARN the processes are Resource Manager, Node Manager, Application Master, and Container.

179) How many yarn daemons processes run on a Hadoop cluster?
a. Resource Manager – Master daemon process. &  b. Node Manager - one slave daemon process per node in a cluster

181) Ratio between heap memory for mapper & reducer (Repeat)
Ans. The Ratio between heap memory for mapper & reducer is 2:1

182) Have you worked in BDR?
Ans:Yes

183) How will you check a process ID of a service?
Ans: Using top command either grep particular service u will get pid for that process.

184) How will you check swapiness memory?
Ans: cat /proc/sys/vm/swappiness

185) 
How will drop swapiness memory?
Ans: Set it to zero (0)

186) Location where ranger stores policy

Ans:The user or group information is stored within Ranger portal and used for policy definition.

187) How will you copy a huge file of size 80GB into HDFS parallelly? 
Using the distCP tools huge files can be copied within or in between various Hadoop clusters. 

188) How will you manually enter and leave Safe Mode in Hadoop? 
$ Hdfs dfsadmin -safe mode enter  and  $ Hdfs dfsadmin -safe mode leave 

189) Tell me how to like go to ui of namenode?
A. select nanmenode publice ip open new tab paste there and with namenode port number 50070. and you will see namenode UI.

190) When Name Node is down, what does the Job Tracker do? (repeat)
When Name node is down the entire cluster is inaccessible, hence the job tracker will also be down.

191) What happens when the mappers fail when a job is run?
Answer:	The JobTracker will attempt for more 4 times and they fail the job.

192) What is Shuffling in MapReduce?
The process of transferring data (outputs) from mapper to the reducers is known as Shuffling.

193) how many types of cloudera manager?
Ans:
1) basic edition 2) flex edition 3) cloudrea enterprise

194) What is the default port number used to log in to the Ambari Server Web UI?
Port 8080

195) What is an advantage of starting with a small pilot cluster?
It may be used for benchmark testing to help determine the proper size of hardware resources

196) What is a key consideration when planning for master nodes?
Availability 

197) What is a key consideration when planning for worker nodes?
Throughput

198) What are key considerations when planning the network?
Avoiding single points of failure and ensuring adequate bandwidth 

199) What Name Node HA function do the Journal Nodes provide?
They provide shared edits log files to the two Name Nodes.

200) In Name Node HA, a Data Node sends its block reports to which Name Node?
To both Name Nodes

201) What is function of jobtracker
ans:- client submits the job to job tracker and it submits the work to the particular task tracker.

202) Which two Apache frameworks can be used to automate HDFS backups?
 Apache Falcon and Apache Oozie.

203) Which data do we query using Hive?
Answer:	Structured data.

204) List some use cases of the Hadoop Ecosystem
Text Mining, Graph Analysis, Semantic Analysis, Sentiment Analysis, Recommendation Systems, predictive analysis etc

205) What is Pig?
Answer:	It is a ETL tool and it’s a procedural language.

206) Mention what are the data components used by Hadoop?
Data components used by Hadoop are
•	Pig
•	Hive

207) Have you configured HBase on a cluster, Are you working on a hbase?
Ans;- no i havent got the opportunity to do it but i am ready to learn if i get an opportunity.

208) What are component of hive ? 
Ans:- user iterface, hive metastor, hiveql process engine, execution engine, hdfs or hbase.

209) For Map what is the default heap memory
Ans.  For Map the default heap memory is 2.5 GB & for reducer is 1.5 GB

210) Suppose Hadoop spawned 100 tasks for a job and one of the task failed. What will Hadoop do?
Ans. 
- It will restart the task again on some other TaskTracker and only if the task fails more than four ( the default setting and can be changed) times will it kill the job.

211) How to process diffeerent file by different mappers?
Answer:
- Multiplelnputs class supports mapreduce jobs that have multiple input path with a different InputFormat adn mapper for each path.

212) Suppose CM is down,it will take 24hour for trouble shooting how will you monitor.
Ans:We manage the alerts using ngios and also prepare the scripts for checking the status of various services runing in our cluster.

213) Suppose a disk is down out of 10 what will you do.
Ans: Not an issue if the disk is down from the same rack the other disk have replica of the data and client get his replica from other disk.

214) How will you come to know that data node is down?
Answer:	When a data node goes down you come to know on the monitoring dashboard of cloudera. Or by jps, or a unix bootstrap I guess.

215) Sentry is used at user level or groups level?
Ans:Sentry is used for User level. Allows the user to see only those tables and databases for which this user has privileges.

216) What do the four V’s of Big Data denote? 
IBM has a nice, simple explanation for the four critical features of big data:
a) Volume –Scale of data
b) Velocity –Analysis of streaming data
c) Variety – Different forms of data
d) Veracity –Uncertainty of data

217) What does /etc/init.d do?
Ans. 
- /etc /init.d specifies where daemons (services) are placed or to see the status of these daemons. 
- It is very LINUX specific, and nothing to do with Hadoop.

218) What are the most commonly defined input formats in Hadoop?
Answer:
The most common input formats defined in Hadoop are;
•	TextInputFormat
•	KeyValueInputFormat
•	SequenceFileInputFormat

219) For a job in Hadoop, is it possible to change the number of mappers to be created?
No, it is not possible to change the number of mappers to be created. The number of mappers is determined by the number of input splits.

220) Mention what is the next step after Mapper or MapTask?
The next step after Mapper or MapTask is that the output of the Mapper are sorted, and partitions will be created for the output.

221) Explain what does the conf.setMapper Class do ?
Conf.setMapperclass  sets the mapper class and all the stuff related to map job such as reading data and generating a key-value pair out of the mapper

222) Explain what is the purpose of RecordReader in Hadoop?
In Hadoop, the RecordReader loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper.

223) What is iptables?
A. internal firewall. Firewall is two types internal and external.
Internal is iptables. (my own server)
External is squid. (company)

224) Can we search for files using wildcards?
Ans. 
- Yes. For example, to list all the files which begin with the letter a, you could use the ls command with the * wildcard &minu; hdfs dfs –ls a*

225) Is YARN a replacement of Hadoop Map Reduce?
YARN is not a replacement of Hadoop but it is a more powerful and efficient technology that supports Map Reduce and is also referred to as Hadoop 2.0 or Map Reduce 2.

226) What is yarn capability ?
Ans.Yarn opens up Hadoop to data-processing by separating concerns of cluster resource management from data processing. This makes a lot of new projects possible.

227) While copying would you require 50070 port as it would take you directly to namenode
If you are using 50070 it will directly point you to the UI of that cluster’s namenode, but if you have to use the HDFS part you have to use 8020.

228)How to keep HDFS cluster balanced? (Yaad nahi)
Answer:
Balancer is a tool that tries to provide a balance to a certain threshold among data nodes by copying block data distribution across the cluster.

229) How can i renew my ticket lifetime?

Answer: 
On all the KDC servers, set the following parameter under "[realms]" in /etc/krb5kdc/krb5.conf file and restarted the KDC daemon:
ticket_lifetime = 2d  
renew_lifetime = 7d

230) What is command to install kdc server?
Answer:
a) Centos: yum install krb5-server krb5-libs krb5-workstation 
b) Ubnutu: apt-get install krb5-server krb5-libs krb5-workstation

231) Mention what daemons run on a master node and slave nodes?
•	Daemons run on Master node is “NameNode”
•	Daemons run on each Slave nodes are “Task Tracker” and “DataNode”

232) Explain how can you debug Hadoop code?
The popular methods for debugging Hadoop code are:
•	By using web interface provided by Hadoop framework
•	By using Counters

233) What Is "fsck" And What Is Its Use?
Answer :
"fsck" is File System Check. FSCK is used to check the health of a Hadoop Filesystem. It generates a summarized report of the overall health of the filesystem. 

234) Are Job Tracker and Task Tracker present on the same machine? 
No, they are present on separate machines as Job Tracker is a single point of failure in Hadoop Map Reduce and if the Job Tracker goes down all the running Hadoop jobs will halt. 

235) How do you define “block” in HDFS? What is the block size in Hadoop 1 and in Hadoop 2?
Answer:
A “block” is the minimum amount of data that can be read or written. 
Hadoop 1 default block size: 64 MB
Hadoop 2 default block size:  128 MB

236) About openjdk, ibmjdk, sunjdk, oraclejdk/
Answer:	
•	Openjdk	-	Belongs to opensource community.
•	Oraclejdk	-	Belongs to Oracle
•	Sunjdk		-	Belongs to sun microsystems.
•	Ibmjdk		-	Belongs to IBM
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------