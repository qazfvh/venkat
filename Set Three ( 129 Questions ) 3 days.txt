Set One
======
1) How does hadoop know how many mappers has to be started?
Answer:
- Number of mappers equals number of input splits.
- Number of input splits ( for single file) = ceil (size of file) / (size of input split)
- if have 1 GB data  means 1GB/ 128MB = 8 mappers.

2) What are the similarities and differences between flume and sqoop?
Answer:
- Both are data ingestion tool.
- Sqoop is specialised to bring data to and from RDBMS and HDFS, where as
  Flume is designed to collect data from diverse sources to HDFS.
- Sqoop job or import/export process runs only once unless scheduled by a scheduler
  Whereas Flume is long running agent on its own.

3) What is Fault Tolerance?
Answer: 
- Suppose you have a file stored in a system, and due to some technical problem that file gets destroyed. 
- Then there is no chance of getting the data back present in that file. 
- To avoid such situations, Hadoop has introduced the feature of fault tolerance in HDFS. 
- In Hadoop, when we store a file, it automatically gets replicated at two other locations also. 

4) Replication causes data redundancy, then why is it pursued in HDFS?
Answer:
- HDFS works with commodity hardware that has high chances of getting crashed any time. 
- Thus, to make the entire system highly fault-tolerant, HDFS replicates and stores data in different places. 
- Any data on HDFS gets stored at least 3 different locations. 
- So, even if one of them is corrupted and the other is unavailable for some time for any reason, then data can be accessed from the third one. 

5) Since the data is replicated thrice in HDFS, does it mean that any calculation done on one node will also be replicated on the other two?
- No, calculations will be done only on the original data. 
- The master node will know which node exactly has that particular data. 
- In case, if one of the nodes is not responding, it is assumed to be failed. 
- Only then, the required calculation will be done on the second replica.

6) How client application interacts with the NameNode? OR How The Client Communicates With Hdfs? 
Answer:
1. Client applications interact using Hadoop HDFS API with the NameNode when it has to locate/add/copy/move/delete a file.
2. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data is residing.
3. Client can talk directly to a DataNode after the NameNode has given the location of the data

7) What Is A Namenode? How Many Instances Of Namenode Run On A Hadoop Cluster?
Answer :
- The NameNode is the centerpiece of an HDFS file system. 
- It is the master node on which job tracker runs and consists of the metadata. 
- It maintains and manages the blocks which are present on the datanodes. 
- It is a high-availability machine and single point of failure in HDFS.
- There is only One NameNode process run on any hadoop cluster. NameNode runs on its own JVM process.

8) What Is A Datanode? How Many Instances Of Datanode Run On A Hadoop Cluster?
Answer :
- A DataNode stores data in the Hadoop File System HDFS. 
- Datanodes are the slaves which are deployed on each machine and provide the actual storage. 
- These are responsible for serving read and write requests for the clients.
- DataNode instances can talk to each other, this is mostly during replicating data.
- There is only One DataNode process run on any hadoop slave node. DataNode runs on its own JVM process.  

9) How The Client Communicates With Hdfs? OR How client application interacts with the NameNode?
Answer :
- The Client communication to HDFS happens using Hadoop HDFS API. 
- Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file on HDFS. 
- The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives. 
- Client applications can talk directly to a DataNode, once the NameNode has provided the location of the data.

10) If Datanodes Increase, Then Do We Need To Upgrade Namenode? *
Answer :
While installing the Hadoop system, Namenode is determined based on the size of the clusters. 
Most of the time, we do not need to upgrade the Namenode because it does not store the actual data, but just the metadata, so such a requirement rarely arise.

11) What If Rack 2 And Datanode Fails?
Answer :
- If both rack2 and datanode present in rack 1 fails then there is no chance of getting data from it. 
- In order to avoid such situations, we need to replicate that data more number of times instead of replicating only thrice. 
- This can be done by changing the value in replication factor which is set to 3 by default.

12) If the hardware quality of few machines in a Hadoop Cluster is very low. How will it affect the performance of the job and the overall performance of the cluster?
There would be an impact on the performance of the jobs. 
Eg: we have 10 machines in a cluster and out of which 2 have poor hardware quality now when the jobs are submitted the tasks on the 8 machines will complete faster however 
due to 2 machines slow performance the entire job will be delayed.

13) Explain Name Node?
Name Node: 
Name Node is at the heart of the HDFS file system which manages the metadata i.e. the data of the files is not stored on the Name Node 
but rather it has the directory tree of all the files present in the HDFS file system on a hadoop cluster. 
Name Node uses two files for the namespace- Fsimage file and Edits file
Fsimage file- It keeps track of the latest checkpoint of the namespace.
Edits file-It is a log of changes that have been made to the namespace since checkpoint.

14) Which Are The Three Main Hdfs-site.xml Properties?
Answer :
The three main hdfs-site.xml properties are:
Dfs.name.dir which gives you the location on which metadata will be stored and where DFS is located.
Dfs.data.dir which gives you the location where the data is going to be stored.
Fs.checkpoint.dir which is for secondary Namenode

15) Tell Us What Cloudera Is And Why It Is Used In Big Data?
Answer :
Cloudera is the leading Hadoop distribution vendor on the Big Data market, 
Its termed as the next-generation data management software that is required for business critical data challenges 
that includes access, storage, management, business analytics, systems security, and search

16) What Is The Function Of Hadoop-env.sh? Where Is It Present?
Answer :
This file contains some environment variable settings used by Hadoop; 
It provides the environment for Hadoop to run. 
The path of JAVA_HOME is set here for it to run properly. 
Hadoop-env.sh file is present in the conf/hadoop-env.sh location. 

17) What is fstab 
do you know about scripting?
Ans:
- fstab is a system configuration file on Linux and other Unix-like operating systems that contains information about major filesystems on the system. ...
- The term filesystem can refer to a hierarchy of directories (also called a directory tree) that is used to organize files on a computer system.
- yes it is automate our most of the work using scripting.

18) Explain what is “map” and what is “reducer” in Hadoop?
Answer:
In Hadoop, A map is a phase in HDFS query solving.  A map reads data from an input location, and outputs a key value pair according to the input type.
In Hadoop, A reducer is collects the output generated by the mapper, processes it, and creates a final output of its own.

19) Mention what is the use of Context Object?
The Context Object enables the mapper to interact with the rest of the Hadoop system.
It includes configuration data for the job, as well as interfaces which allow it to emit output.

20) Mention what is rack awareness
Rack awareness is the way in which the namenode determines on how to place blocks based on the rack definitions.
All the data nodes put together form a storage area i.e. the physical location of the data nodes is referred to as Rack in HDFS. 
The rack information i.e. the rack id of each data node is acquired by the NameNode. 

21) What are security measure you have to taken for your cluster.?
Ans:
1. Plan before you deploy
2. Don’t overlook basic security measures 
3. Choose the right remediation technique 
4. Ensure that encryption integrates with access control 
5. Monitor, detect and resolve issues.
We have many choices to secure our cluster as per cloudera and hortonworks.
In case of cloudera we use AD kerberos &
In case of Hortonworks we use Ranger

22) How to control number of reducers in a mapreduce program?
Answer:
- By default, for every 1GB of input data 1 reducer is created. But it can be overridden
- You can set number of   reducer on job object there is function call as job.setNumReduceTask (int n) 
- It is specify how many reducer you want that particular mapreducer application.

23) What is rack-aware replica placement policy?
Answer:
1. Rack-awareness is used to take a node’s physical location into account while scheduling tasks and allocating storage.
2. Default replication factor is 3 for a data blocks on HDFS.
3. The first two copies are stored on DataNodes located on the same rack while the third copy is stored on a different rack

24) What are the problems with Hadoop 1.0?
Answer:
1. NameNode: No Horizontal Scalability and No High Availability
2. Job Tracker: Overburdened.
3. MRv1: It can only understand Map and Reduce tasks

25)  Is it possible to load 3TB of data in hive ?
Answer:
 Yes,  you can load 3tb of data from your local file system but it will take too much time that all are depends on your local systems configurations like Ram,processor.
 hive meta store only save the metadata of 3tb table and the actual data will stored in hdfs only.

26) Why do the nodes are removed and added frequently in a Hadoop cluster?
Answer:
1. The Hadoop framework utilizes commodity hardware, and it is one of the important features of Hadoop framework. 
    It results in a frequent DataNode crash in a Hadoop cluster.
2. The ease of scale is yet another important feature of the Hadoop framework that is performed according to the rapid growth of data volume.

27) Explain the NameNode recovery process.
Answer:
Step 1: To start a new NameNode, utilize the file system metadata replica (FsImage).
Step 2: Configure the clients and DataNodes to acknowledge the new NameNode.
Step 3: Once the new Name completes the loading of last checkpoint FsImage and receives block reports from the DataNodes, the new NameNode start serving the client.

28) Define “Checkpointing”. What is its benefit?
Answer:
Checkpointing is a procedure to that compacts a FsImage and Edit log into a new FsImage.
In this way, the NameNode handles the loading of the final inmemory state from the FsImage directly, instead of replaying an edit log. 
The secondary NameNode is responsible to perform the checkpointing process.
Benefit of Checkpointing:
Checkpointing is a highly efficient process and decreases the startup time of the NameNode.

29) What happens when two clients try to access the same file in HDFS?
Answer:
- When the n first client contacts the NameNode to open the file to write, the NameNode provides a lease to the client to create this file.
- When the second client sends a request to open that same file to write, the NameNode find that the lease for that file has already been given to another client, and
  thus reject the second client’s request.

30) What is HDFS namenode federation?
Answer:
- In HDFS federation, there are multiple namenodes, each storing metadata and block mapping of files and directories contained in 
   particular sub-directories.
- The list of sub-directories managed by a namenode is called a namespace volume.
- Blocks for files belonging to namespace is called block pool.

31) Can You Elaborate About Mapreduce Job?
Answer :
Based on the configuration, the MapReduce Job first splits the input data into independent chunks called Blocks. 
These blocks processed by Map() and Reduce() functions. 
First Map function process the data, then processed by reduce function. 
The Framework takes care of sorts the Map outputs, scheduling the tasks. 

32) How Does Master Slave Architecture In The Hadoop?
Answer :
The MapReduce framework consists of a single master JobTracker and multiple slaves, each cluster-node will have one TaskTracker. 
The master is responsible for scheduling the jobs' component tasks on the slaves, monitoring them and re-executing the failed tasks. 
The slaves execute the tasks as directed by the master.

33) Following 3 Daemons run on Master nodes.
Answer:
- NameNode : This daemon stores and maintains the metadata for HDFS.
- Secondary NameNode : Performs housekeeping functions for the NameNode.
- JobTracker : Manages MapReduce jobs, distributes individual tasks to machines running the Task Tracker. Following 2 Daemons run on each Slave nodes
- DataNode : Stores actual HDFS data blocks.
- TaskTracker : It is Responsible for instantiating and monitoring individual Map and Reduce tasks.

34) - Why do we need Hadoop?
Ans. 
- Everyday a large amount of unstructured data is getting dumped into our machines. 
- The major challenge is not to store large data sets in our systems but to retrieve and analyze the big data in the organizations, that too data present in different machines at different locations. 
- In this situation a necessity for Hadoop arises. 

35) What Is The Function Of ‘job Tracker’?
Answer :
Job tracker is one of the daemons that runs on name node and submits and tracks the MapReduce tasks in Hadoop. 
There is only one job tracker who distributes the task to various task trackers. 
When it goes down all running jobs comes to a halt

36) Explain how indexing in HDFS is done?
Hadoop has a unique way of indexing. Once the data is stored as per the block size, 
the HDFS will keep on storing the last part of the data which say where the next part of the data will be.

37) How many Data Nodes can be run on a single Hadoop cluster? 
Thousands of data nodes can be run on a single hadoop cluster but there are many factors to consider: 
name node memory, no. of blocks to be stored, block replication factor, how will the cluster be used etc

38) Can we implement RAID on HDFS? Datanode as well as namenode.
Ans:
- RAID is used for two purposes. Depending on the RAID configuration you can get:
Better performance and Fault-tolerance
- Since HDFS is taking care of fault-tolerance and "striped" reading, there is no need to use RAID underneath an HDFS.
- Since the namenode is a single-point-of-failure in HDFS, it requires a more reliable hardware setup. Therefore, the use of RAID is 
   recommended on namenodes.

39) What is DRF tell me about that.
Ans:
An extension of fair scheduling for more than one resource—it determines resource shares (CPU, memory) for a job separately based on the 
availability of those resources and the needs of the job. 
1.If a pool's allocation is not in use it can be given to other pools. 
2.Otherwise, a pool receives a share of resources in accordance with the pool's weight. 

40) Can You Tell Is What Will Happen To A Namenode, When Job Tracker Is Not Up And Running?
Answer :
When the job tracker is down, it will not be in functional mode, all running jobs will be halted because it is a single point of failure. 
Your whole cluster will be down but still Namenode will be present. 
As such the cluster will still be accessible if Namenode is working, even if the job tracker is not up and running. But you cannot run your Hadoop job

41) What is SSL is used for ?
- It is a standard security technology for establishing an encrypted link between a server and a client—typically a web server (website) and a browser.
- After the secure connection is made, the session key is used to encrypt all transmitted data.

42) How is the distance between two nodes defined in Hadoop?
Measuring bandwidth is difficult in Hadoop so network is denoted as a tree in Hadoop. 
The distance between two nodes in the tree plays a vital role in forming a Hadoop cluster  and is defined by the network topology and java interface DNStoSwitchMapping. 
The distance is equal to the sum of the distance to the closest common ancestor of both the nodes. 
The method getDistance(Node node1, Node node2) is used to calculate the distance between two nodes with the assumption that the distance from a node to its parent node is always 1.

43) Explain about the functioning of Master Slave architecture in Hadoop?
Hadoop applies Master Slave architecture at both storage level and computation level.
Master: Name Node
File system namespace management and access control from client.
Executes and manages operation of file system namespace like closing, opening, renaming of directories and files.
Slave: Data Node
A cluster has only one Data Node - per node. File system namespace are exposed to the clients, which allow them to store the files. These files are split into two or more blocks, which are stored in the Data Node. Data Node is responsible for replication, creation and deletion of block, as and when instructed b

44) How are HDFS blocks replicated?
The default replication factor is 3 which means that the data is safe if 3 copies are created. 
HDFS follows a simple procedure to replicate blocks.One replica is present on the machine on which the client is running, the second replica is created on a randomly chosen rack (ensuring that it is different from the first allocated rack) 
and a third replica is created on a randomly selected machine on the same remote rack on which second replica is created.

45) What is InputFormat in Hadoop?
As the name suggests, it specifies the process of reading data from files into an instance of the Mapper. 
There are various implementations of InputFormat, like for reading text files, binary data and etc.We can even create our own custom InputFormat implementation.  
Another important job of InputFormat is to split the data and provide inputs to map tasks.

46) How will you check which stack is there if dont have webUI of CM?
Answer: 
If you see hadoop process is not running on 
i) ps -ef|grep hadoop
ii) start-dfs.sh. 
iii) Monitor with 
hdfs dfsadmin -report

iv) To check whether Hadoop Nodes are running or not:
sudo -u hdfs hdfs dfsadmin -report

47) How will you check ram, cpu, storage?
Answer:
i) CPU
$ cat /proc/cpuinfo

ii) Memory :
$ free
$ cat /proc/meminfo

iii) HDD:
$ df -h
$ sudo fdisk -l
$ hdparm -i /dev/device

48) linux How will you check filesystem?
Answer:
i) Check File System Disk Space Usage
[root@tecmint ~]# df

ii) Display Information of all File System Disk Space Usage
[root@tecmint ~]# df -a

iii) Show Disk Space Usage in Human Readable Format
[root@tecmint ~]# df -h
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Set Two
========

49) Are job tracker and task trackers present in separate machines?
Answer:
They are separate machines because
JobTracker is a master which creates and runs the job. 
JobTracker which can run on the NameNode allocates the job to TaskTrackers which run on DataNodes; 
TaskTrackers run the tasks and report the status of task to JobTracker.

50) How NameNode Handles data node failures?
Answer:
DataNode will constantly send a heartbeat to Name node in this way Name node understands that Data node is working ,
If in case (due to any reason) Data node stops sending the heartbeat to the Name node ,
Then name node will come to know that that particular Data node is down and then make sure that the Blocks in that Data node get replicated in another node and 
If in case the node which stopped sending the heartbeat again started to send its heartbeat then Name node will balance the replication factor again 

51) If RM goes down, will jobs continue or all jobs will get fail?
Answer:
When the Active goes down or becomes unresponsive, another RM is automatically elected to be the Active which then takes over. 
Note that, there is no need to run a separate ZKFC daemon as is the case for HDFS because ActiveStandbyElector embedded in RMs acts as a failure detector and a leader elector instead of a separate ZKFC deamon.
ResourceManager represented a single point of failure—if NodeManagers lost contact with the ResourceManager, all jobs in progress would be halted, and no new jobs could be assigned.
The new feature incorporates ZooKeeper to allow for automatic failover to a standby ResourceManager in the event of the primary’s failure

52) Can you schedule BDR for later time? how?
Answer:
Yes, using cloudera manager server we can schedule bdr later time with replication of hdfs within the cluster or into the another cluster
Steps
- Configuring Replication of HDFS Data
- Viewing Replication Schedules
- Viewing Replication History

53) Explain about Capacity scheduler?

Answer:
It assigns resource based on the capacity required by the organisation. 
This is set up by queues for each organisation with specified amount of capacity. 
The queue is based on FIFO scheduling.

54) have you performed yarn tunning
 ? OR
What are the parameters that needs to be tweaked to fine-grain the performance?

Answer:
1) Memory Tuning
2) Improving IO Performance
3) Minimizing the Disk Spill by Compressing Map Output
4) Tuning the Number of Mapper or Reducer Tasks
&
yarn.app.mapreduce.am.resource.cpu-vcores
yarn.app.mapreduce.am.resource.mb
mapreduce.map.cpu.vcores
mapreduce.map.memory.mb
mapreduce.reduce.cpu.vcores
mapreduce.reduce.memory.mb
mapreduce.reduce.java.opts
mapreduce.task.io.sort.mb (disk spill)

55) Do you know log file concept?
Ans. A log file is a recording of everything that goes in and out of a particular server. 
The point of a log file is to keep track of what is happening with the server. 
If something should malfunction within a complex system, there may be no other way of identifying the problem. 
Log files are also used to keep track of complex systems, so that when a problem does occur, it is easy to pinpoint and fix. 
Log files are also important to keeping track of applications that have little to no human interaction, such as server applications

56) What is checkpointing ?
Ans. Checkpointing is an essential part of maintaining  filesystem metadata in HDFS. 
It’s crucial for efficient NameNode recovery and restart, and is an important indicator of overall cluster health. 
Checkpointing is a process that takes an fsimage and edit log and compacts them into a new fsimage

57) Secondary NN is installed on the same master node or on another machine ?
Ans. It is always better to deploy a secondary NameNode on a separate machine. 
When the secondary NameNode is deployed on a separate machine it does not interfere with the operations of the primary node.

58) Explain Fsimage & edit logs
Ans.  fsimage – An fsimage file contains the complete state of the file system at a point in time. 
Every file system modification is assigned a unique transaction ID. An fsimage file represents the file system state after all modifications up to a specific transaction ID.
edits – An edits file is a log that lists each file system change (file creation, deletion or modification) that was made after the most recent fsimage.

59) Do you have anything called over replicated in Hadoop?
Answer:	Yes for example if your one node goes down HDFS will replicate the blocks residing on that node on another node. 
When that node which was down starts again all the blocks on that becomes available again. 
In this scenario the blocks will get over replicated as they were replicated assuming the node was down.

60) Differentiate between Structured and Unstructured data.
In Structured Data,  Data which can be stored in traditional database systems in the form of rows and columns, 
In Unstructured data, Unorganized and raw data that cannot be categorized as semi structured or structured data is referred to as unstructured data

61) For a Hadoop job, how will you write a custom partitioner?
You write a custom partitioner for a Hadoop job, you follow the following path
•	Create a new class that extends Partitioner Class
•	Override method getPartition
•	In the wrapper that runs the MapReduce
•	Add the custom partitioner to the job by using method set Partitioner Class or – add the custom partitioner to the job as a config file

62) What is heap memory ?
Ans. Heap is a portion of memory where dynamically allocated memory resides. 
Memory allocated from the heap will remain allocated until (i) the memory is free’d or (ii) the program terminates. 
It is a chunk of memory allocated from the operating system by the memory manager. Blocks of memory are allocated & freed in a random order. 
Heap is usually used by a program for different purpose.

63) - What is a rack?
Ans. 
- Rack is a storage area with all the datanodes put together. 
- These datanodes can be physically located at different places. Rack is a physical collection of datanodes which are stored at a single location. 
- There can be multiple racks in a single location.

64) What is the use of warehouse-dir in sqoop import?
Answer:
- warehouse-dir is the HDFS parent directory for table destination.
- If we specify target-dir, all our files are stored in that location, but with warehouse-dir, a child directory is created inside it with the
  name of the table.
- All the files are stored inside that child directory.

65) Why HDFS is not used by the Hive metastore for storage?
Answer:
- Editing files or data present in HDFS is not allowed.
- metastore stores metadata using RDBMS to provide low query latency
- HDFS read/write operations are time cosuming processes.

66) What happens when a datanodefails ?
When a datanode fails
• Jobtracker and namenode detect the failure
• On the failed node all tasks are re-scheduled
• Namenode replicates the users data to another node

67) Hdfs Balancer
? 
Ans:
- HDFS data might not always be distributed uniformly across DataNodes. 
- One common reason is addition of new DataNodes to an existing cluster. 
- HDFS provides a balancer utility that analyzes block placement and balances data across the DataNodes. 
- The balancer moves blocks until the cluster is deemed to be balanced, which means that the utilization of every DataNode differs from the utilization of the cluster by no more than a given threshold percentage. 
- The balancer does not balance between individual volumes on a single DataNode

68) How will checkpointing work when we have NN HA?
Ans. Checkpointing with a Standby NameNode
When NameNode high-availability is configured, the active and standby NameNodes have a shared storage where edits are stored. 
Typically, this shared storage is an ensemble of three or more JournalNodes, but that’s abstracted away from the checkpointing process.
The standby NameNode maintains a relatively up-to-date version of the namespace by periodically replaying the new edits written to the shared edits directory by the active NameNode. 
As a result, checkpointing is as simple as checking if either of the two preconditions are met, saving the namespace to a new fsimage (roughly equivalent to running hdfs dfsadmin -saveNamespace on the command line), then transferring the new fsimage to the active namenode

69)  What are the core changes in Hadoop 2.0?
Hadoop 2.x provides an upgrade to Hadoop 1.x in terms of resource management, scheduling and the manner in which execution occurs. 
In Hadoop 2.x the cluster resource management capabilities work in isolation from the Map Reduce specific programming logic. 
This helps Hadoop to share resources dynamically between multiple parallel processing frameworks like Impala and the core Map Reduce component. Hadoop 2.x Hadoop 2.x allows workable and fine grained resource configuration leading to efficient and better cluster utilization so that the application can scale to process larger number of jobs.

70) On what concept the Hadoop framework works? 
Hadoop Framework works on the following two core components-
1) HDFS – Hadoop Distributed File System is the java based file system for scalable and reliable storage of large datasets. Data in HDFS is stored in the form of blocks and it operates on the Master Slave Architecture.
2) Hadoop Map Reduce-This is a java based programming paradigm of Hadoop framework that provides scalability across various Hadoop clusters. Map Reduce distributes the workload into various tasks that can run in parallel. Hadoop jobs perform 2 separate tasks- job. The map job breaks down the data sets into key-value pairs or tuples. The reduce job then takes the output of the map job and combines the data tuples to into smaller set of tuples. The reduce job is always performed after the map job is executed.

71) Explain the differences between Hadoop 1.x and Hadoop 2.x
In Hadoop 1.x, Map Reduce is responsible for both processing and cluster management whereas in Hadoop 2.x processing is taken care of by other processing models and YARN is responsible for cluster management.
Hadoop 2.x scales better when compared to Hadoop 1.x with close to 10000 nodes per cluster.
Hadoop 1.x has single point of failure problem and whenever the Name Node fails it has to be recovered manually. However, in case of Hadoop 2.x Standby Name Node overcomes the SPOF problem and whenever the Name Node fails it is configured for automatic recovery.
Hadoop 1.x works on the concept of slots whereas Hadoop 2.x works on the concept of containers and can also run generic tasks.

72) Mention what is the difference between an RDBMS and Hadoop?
RDBMS	                                                                                                                                                                          Hadoop
RDBMS is relational database management system	                                                                                     Hadoop is node based flat structure
It used for OLTP processing whereas Hadoop	                                                                                     It is currently used for analytical and for BIG DATA processing
In RDBMS, the database cluster uses the same data files stored in shared storage	                     In Hadoop, the storage data can be stored independently in each processing node.
You need to preprocess data before storing it	                                                                                     You don’t need to preprocess data before storing it

73) Suppose i want to  decommission 2 nodes how n what will i do?
1. Update the network addresses in the 'exclude' files dfs.exclude and mapred.exclude
2. Update the Namenode  hadoop dfsadmin -refreshNodes   
3. Update the Jobtracker hadoop mradmin -refreshNodes
4. Check Web UI it will show “Decommissioning in Progress” "Decommissioned".
5. Remove the Nodes from include file and then run hadoop dfsadmin -refreshnodes and hadoop mradmin -refreshnodes.
6. Remove the Nodes from slave file

74) Suppose i want to  commission 2 nodes how n what will i do?
Answer: 
- Update the network addresses in the 'include' files dfs.include and mapred.include
- Update the Namenode hadoop dfsadmin -refeshnodes
- Update the jobtracker hadoop mradmin -refreshnodes
- Update the 'slaves' file
- Start the datanodes and task tracker hadoop-deamon.sh start datanode and hadoop-deamon.sh start tasktracker
- Cross check the web UI to ensure the successful addition
- Run balancer to move the HDFS block to datanodes.

75) What are the Different ways to restart NameNode in Hadoop? OR
What is best way to restart all the daemons in Hadoop?
Answer:
First stop

$HADOOP_HOME/bin/stop.dfs.sh
$HADOOP_HOME/bin/hadoop-daemon.sh stop namenode

Then start again

$HADOOP_HOME/bin/start.dfs.sh
$HADOOP_HOME/bin/hadoop-daemon.sh start namenode

To restart all daemons use the following command which deprecated

$HADOOP_HOME/bin/stop.all.sh
$HADOOP_HOME/bin/start.all.sh

76) How to restart NameNode or all the daemons in Hadoop? OR
What is the best way to restart all the daemons in Hadoop?
Answer:
There are three ways to start the Daemons in Hadoop

1. start-all.sh & stop-all.sh : To start/stop all the deamons on all the nodes from Master machine all at once.

2. start-dfs.sh, stop-dfs.sh : to start/ stop HDFS daemons separately on all the nodes from Master machine

3. start-yarn.sh, stop-yarn.sh : : to start/ stop Yarn daemons separately on all the nodes from Master machine

77) how do you get data from sqoop and what are the steps?
Answer:
A) The import tool imports individual tables from RDBMS to HDFS.
- When we submit Sqoop command, our main task gets divided into sub tasks which is handled by individual Map Task internally.
B) The export tool exports a set of files from HDFS back to an RDBMS.
- When we submit our Job, it is mapped into Map Tasks which brings the chunk of data from HDFS. These chunks are exported to a structured data destination.
- Combining all these exported chunks of data, we receive the whole data at the destination, which in most of the cases is an RDBMS (MYSQL/Oracle/SQL Server).
-Reduce phase is required in case of aggregations. But, Apache Sqoop just imports and exports the data; it does not perform any aggregations

78) If i want to give access to one user to use other users file how will i do it?
Answer:
- Create a New User
- Change the permission of a directory in HDFS (Hadoop Distributed FileSystem) where hadoop stores its temporary data.
- Now, give write permission to the user group on hadoop.tmp.dir.To get the path for hadoop.tmp.dir open core-site.xml 
- Now Create user home directory in HDFS-
- Superuser has the ownership of newly created directory structure.But the new user will not be able to run MapReduce programs with this. So, to achieve this, change the ownership of newly created directory in HDFS to the new user

79) What are managed tables and external tables in hive?
Answer:
- In HIVE there are two ways to create tables: Managed Tables and External Tables
- HIVE by default manages the data and saves it in its own warehouse
- we can also create an external table, which is at an existing location outside the HIVE warehouse directory
- To create Managed tables, we just use the simple CREATE Statement. When we load a data into a Managed table, it is moved into HIVE warehouse directory.
- EXTERNAL TABLES, the location of the external data is specified at table creation time and also uses the Key word EXTERNAL in CREATE STATEMENT
- the data is not moved to HIVE warehouse directory, and it is actually saved in an external location, therefore when you drop the external_table, HIVE will leave the data untouched and only delete the metadata

80) What is my namenode is down and standby namenode is also not coming up, what can b the issue?
Answer:
- standby namenode and journal node configurations were in a corrupted state, so that when the cluster tried to switch to the standby, you encountered the error that you reported.
- Initially we have toW put the primary namenode into safemode and saved the namespace with the following commands:

hdfs dfsadmin -safemode enter hdfs dfsadmin -saveNamespace

su - hdfs -c "hdfs namenode -bootstrapStandby -force"
- this was to make sure that the namenode was in a consistent state before we attempted to restart the HDFS components one last time to make sure all processes started cleanly and that HDFS would automatically leave safemode

OR

1. Put Active NN in safemode

sudo -u hdfs hdfs dfsadmin -safemode enter
2. Do a savenamespace operation on Active NN


sudo -u hdfs hdfs dfsadmin -saveNamespace
3. Leave Safemode

sudo -u hdfs hdfs dfsadmin -safemode leave
4. Login to Standby NN

5. Run below command on Standby namenode to get latest fsimage that we saved in above steps.

sudo -u hdfs hdfs namenode -bootstrapStandby -force

81) First, add the new node's DNS name to the conf/slaves file on the master node.
Ans. 
- Then log in to the new slave node and execute 
- $ cd path/to/hadoop, $ bin/hadoop-daemon.sh start datanode, 
- $ bin/hadoop-daemon.sh start tasktracker , then issuehadoop dfsadmin -refreshNodes and hadoop mradmin -refreshNodes 
so that the NameNode and JobTracker know of the additional node that has been added.

82) - What happens if one Hadoop client renames a file or a directory containing this file while another client is still writing into it?
Ans. 
- A file will appear in the name space as soon as it is created. 
- If a writer is writing to a file and another client renames either the file itself or any of its path components, 
then the original writer will get an IOException either when it finishes writing to the current block or when it closes the file.

83) How to make a large cluster smaller by taking out some of the nodes?
Ans. 
- Hadoop offers the decommission feature to retire a set of existing data-nodes. 
- The nodes to be retired should be included into the exclude file, and the exclude file name should be specified as a configuration parameter dfs.hosts.exclude. 
- The decommission process can be terminated at any time by editing the configuration or the exclude files and repeating the -refreshNodes command

84) What happens when two clients try to write into the same HDFS file?
Ans. 
- HDFS supports exclusive writes only. When the first client contacts the name-node to open the file for writing, the name-node grants a lease to the client to create this file. 
- When the second client tries to open the same file for writing, the name-node will see that the lease for the file is already granted to another client, and will reject the open request for the second client

85) What is a Combiner?
Ans. 
- The Combiner is a 'mini-reduce' process which operates only on data generated by a mapper. 
- The Combiner will receive as input all data emitted by the Mapper instances on a given node. 
- The output from the Combiner is then sent to the Reducers, instead of the output from the Mappers

86) If you get a connection refused exception - when logging onto a machine of the cluster, what could be the reason? How will you solve this issue?
The port is not open on the destination machine.
The port is open on the destination machine, but its backlog of pending connections is full.
A firewall between the client and server is blocking access, to disable use “$sudo ufw disable” (firewall in Ubuntu can’t be disable because it is part of kernel)if this doesn’t work then try “sudo service iptables stop”

87) If the Hadoop services are running slow in a Hadoop cluster, what would be the root cause for it and how will you identify it?
Eg 1: A particular task is using a lot of memory which is causing the slowness or failure, I will look for ways to reduce the memory usage. 
We can also increase the memory requirements needed by the map and reduce tasks by setting – mapreduce.map.memory.mb and mapreduce.reduce.memory.mb

88) Why use hadoop when the amount of incoming data is 30 or 40 GB?
Oracle supports upto 1 TB, however the size of the incoming data is not going to be limited to 30 GB it will be increasing with time 
so there would be a point when oracle is not able to support and then if you go to buy another oracle machine which would cost you around 55,000 dollars. 
Moreover the data which is flowing in is not limited to structured format (only format which is supported by oracle) it contains xml & logs also and we all know that semi-structed & unstructured data can be stored & processed only in hadoop. 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Set Three
=======

89) How can a log file be processed as it is unstructured ?
From github we got the jar file which helped us process this unstructured data. It was like 4 lines command in pig which helped us clean the data. 
Depends on the type of logs you need to select the right jar, after ETL which is done by pig the data is cleaned and it comes in format which can be loaded in hive metastore for the analyst to do the analysis.

90) What is RM,NM, AM, Container ?
Answer: 
1. RM : As previously described, ResourceManager (RM) is the master that arbitrates all the available cluster resources and thus helps manage the distributed applications running on the YARN system. 
2. NM : NodeManagers take instructions from the ResourceManager and manage resources available on a single node.
3. AM : ApplicationMasters are responsible for negotiating resources with the ResourceManager and for working with the NodeManagers to start the containers.
4. Container : It is a combination of Cores and RAM which allow an application to run as all the applications require RAM and cores to run.

91) - What are Problems with small files and HDFS?
Ans. 
- HDFS is not good at handling large number of small files. 
- Because every file, directory and block in HDFS is represented as an object in the namenode's memory, each of which occupies approx 150 bytes So 10 million files, each using a block, would use about 3 gigabytes of memory. 
- when we go for a billion files the memory requirement in namenode cannot be met.

92). - Why is Checkpointing Important in Hadoop?
Ans. 
- As more and more files are added the namenode creates large edit logs. 
- Which can substantially delay NameNode startup as the NameNode reapplies all the edits. 
- Checkpointing is a process that takes an fsimage and edit log and compacts them into a new fsimage. 
- This way, instead of replaying a potentially unbounded edit log, the NameNode can load the final in-memory state directly from the fsimage. 
- This is a far more efficient operation and reduces NameNode startup time.

93) - Why use Bootstrap?
Ans. 
- Bootstrap can be used as - Mobile first approach - Since Bootstrap 3, the framework consists of Mobile first styles throughout the entire library instead of in separate files. 
- Browser Support - It is supported by all popular browsers. Easy to get started - With just the knowledge of HTML and CSS anyone can get started with Bootstrap. 
- Also the Bootstrap official site has a good documentation. Responsive design - Bootstrap's responsive CSS adjusts to Desktops,Tablets and Mobiles. 
- Provides a clean and uniform solution for building an interface for developers. It contains beautiful and functional built-in components which are easy to customize. 
- It also provides web based customization. And best of all it is an open source.

94) Write the full command to creat sentry roles.
Ans:You can crrate roles using command
1.CREATE ROLE `role_name`;
2.CREATE ROLE  `hadoop-developer_role`;

For droping roles.
1.DROP ROLE `role_name`;
2.DROP ROLE  `hadoop-developer_role`;

95) What is kernel
Kernel is the main component of a computer operating systems. 
It provides an interface between applications and actual data processing at the hardware level. 
It is the part of the operating system that loads first, and it remains in main memory. typically, the kernel is responsible for memory management, process and task management, and disk management. If you type a command from the terminal then the kernel interprets that command to the hardware it’s a communicator.

96) How does NameNode tackle DataNode failures?
All the data nodes periodically send notifications a.k.a Heartbeat signal to the NameNode, which implies that the DataNode is alive. 
Apart from Heartbeat, NameNode also receives Block report from DataNodes, which consists of all the blocks on a DataNode. 
In case NameNode does not receive this, it marks that DataNode as a dead node.As soon as the DataNode is marked as non-functional or dead, block transfer is initiated to the DataNode with which replication was done initially.

97) What happens when a datanodefails ?
When a datanode fails
•	Jobtracker and namenode detect the failure
•	On the failed node all tasks are re-scheduled
•	Namenode replicates the users data to another node

98) What are the additional benefits YARN brings in to Hadoop?
Effective utilization of the resources as multiple applications can be run in YARN all sharing a common resource. 
In Hadoop Map Reduce there are separate slots for Map and Reduce tasks whereas in YARN there is no fixed slot. The same container can be used for Map and Reduce tasks leading to better utilization.
YARN is backward compatible so all the existing Map Reduce jobs.
Using YARN, one can even run applications that are not based on the Mar educe model

99) Difference between Application Manager & Application Master.
Ans. Application Master is the main container requesting, launching and monitoring application specific resources, whereas 
Application Manager is a component inside ResourceManager. The ApplicationsManager is responsible for maintaining a collection of submitted applications. 
After application submission it first validates the application’s specifications and rejects any application if there is no node in the cluster that has enough resources to run the ApplicationMaster itself. 
It then ensures that no other application was already submitted with the same application ID—This component is also responsible for recording and managing finished applications for a while before they are completely evacuated from the ResourceManager’s memory.

100) What is Apache Pig?  
It is a high-level platform for extracting, transforming, or analyzing large datasets. 
Pig includes a scripted, procedural-based language that excels at building data pipelines to aggregate and add structure to data. Pig also provides data analysts with tools to analyze data.

101) What is Apache Hive? 
It is a data warehouse infrastructure built on top of Hadoop. It was designed to enable users with database experience to analyze data using familiar SQL-based statements. 
Hive includes support for SQL:2011 analytics. Hive and its SQL-based language enable an enterprise to utilize existing SQL skillsets to quickly derive value from a Hadoop deployment.

102) What is Apache Flume? 
It is a distributed, reliable, and available service that efficiently collects, aggregates, and moves streaming data. It is a distributed service because it can can be deployed across many systems. 
The benefits of a distributed system include increased scalability and redundancy. It is reliable because its architecture and components are designed to prevent data loss. It is highly-available because it uses redundancy to limit downtime.

103) What is Apache Sqoop? 
It is a collection of related tools. The primary tools are the import and export tools. Writing your own scripts or MapReduce program to move data between Hadoop and a database or an enterprise data warehouse is an error prone and non-trivial task. 
Sqoop import and export tools are designed to reliably transfer data between Hadoop and relational databases or enterprise data warehouse systems. 

104) What is Apache Kafka? 
It is a fast, scalable, durable, and fault-tolerant publish-subscribe messaging system. 
Kafka is often used in place of traditional message brokers like Java Messaging Service (JMS) or Advance Message Queueing Protocol (AMQP) because of its higher throughput, reliability, and replication. 

105) What is Apache Knox?
It  is a perimeter gateway protecting a Hadoop cluster. It provides a single point of authentication into a Hadoop cluster.

106) What is Apache Ranger?
It is a centralized security framework offering fine-grained policy controls for HDFS, Hive, HBase, Knox, Storm, Kafka, and Solr. 
Using the Ranger Console, security administrators can easily manage policies for access to files, directories, databases, tables, and columns. 
These policies can be set for individual users or groups and then enforced within Hadoop.

107) Explain what combiners is and when you should use a combiner in a MapReduce Job?
To increase the efficiency of MapReduce Program, Combiners are used.  
The amount of data can be reduced with the help of combiner’s that need to be transferred across to the reducers. 
If the operation performed is commutative and associative you can use your reducer code as a combiner.  
The execution of combiner is not guaranteed in Hadoop

108) Mention what is the HadoopMapReduce APIs contract for a key and value class?
For a key and value class, there are two HadoopMapReduce APIs contract
•	The value must be defining the org.apache.hadoop.io.Writable interface
•	The key must be defining the org.apache.hadoop.io.WritableComparable interface

109) Have you worked in kafka, impala, cassandra, hbase ?
Answer:	No. All I know about them is:
1. Kafka: It is a messaging broadcast in a distributed cluster.
2. Impala: It is a in memory solutions. 
3. Cassandra: It is a distributed database like Teradata and hdfs
4. HBase: It is a columnar store RDBMS lik

110) Are you working in hive?
A.
# First we have use beeline we don't have use hive.
# Second in our enviroment we use run all queries hue browser this is taken care by applications team and my job is secured that monitoring, backup, recovery, trobalshooting, governance,  security and secureing the cluster.
# Producation is always on beeline never have on hive.

111) Do you know zookeeper ?
Ans. It’s a Coordination service for Distributed Applications. Distributed applications are difficult to coordinate and work with as they are much more error prone due to huge number of machines attached to network. 
Each time they are implemented there is a lot of work that goes into fixing the bugs.

112) Explain Name Node HA
Ans. In each cluster, two separate machines are configured as NameNodes. In a working cluster, one of the NameNode machine is in the Active state, and the other is in the Standby state.
The Active NameNode is responsible for all client operations in the cluster. The Standby NameNode maintains enough state to provide a fast failover. In order for the Standby node to keep its state synchronized with the Active node, both nodes communicate through a group of separate daemons called JournalNodes. The file system journal logged by the Active NameNode at the JournalNodes is consumed by the Standby NameNode to keep it’s file system namespace in sync with the Active.
In order to provide a fast failover, it is also necessary that the Standby node have up-to-date information of the location of blocks in the cluster. DataNodes are configured with the location of both the NameNodes and send block location information and heartbeats to both NameNode machines.
The ZooKeeper Failover Controller (ZKFC) is responsible for HA Monitoring of the NameNode service and for automatic failover when the Active NameNode is unavailable. There are two ZKFC processes – one on each NameNode machine. ZKFC uses the Zookeeper Service for coordination in determining which is the Active NameNode and in determining when to failover to the Standby NameNode.
Quorum journal manager (QJM) in the NameNode writes file system journal logs to the journal nodes. A journal log is considered successfully written only when it is written to majority of the journal nodes. Only one of the Namenodes can achieve this quorum write. In the event of split-brain scenario this ensure that the file system metadata will not be corrupted by two active NameNodes.

113) Tunning of yarn how will you tune yarn jobs.
By setting some paramters for the yarn tuning we need to increase .

Ans:1.yarn.nodemanager.resource.memory-mb
2.yarn.nodemanager.resource.cpu-vcores
3.mapreduce.map.memory.mb
4.mapreduce.reduce.memory.mb

Ans:Using RM resource portal we manages the Jobs along with counters shows the jobs are stucked or not .
By setting some paramters for the yarn tuning we need to increase .

114) I want to create one Kerberos  principal which will work same as HDFS user that means it will have a full access of cluster how will I create
Ans: you have to create a user and make his group and supergroup hdfs.once done create a principle with that user name then that
 user will have full authorisation as it is from usergroup of HDFS.

115) What is namenode (HDFS) high availability?
Answer:
- There is a pair of namenodes in active-standby configuration.
- In the event of the failure of the active name, the standby takes over its duties without a significant interruption.
- The namenodes must use highly-available shared storage to share the edit log.
- Edit logs are read by Standbynamenode when it takes the responsibility of Activenamenode.
- Datanodes must send block reports to both namenodes because the block mappings.
- Checkpointing is done by Standby namenode.

116) Why do we use HDFS for applications having large data sets and not when there are lot of small files?
Answer:
- This is because Namenode is a very expensive high performance system, so it is not prudent to occupy the space in the Namenode by unnecessary amount of metadata that is generated for multiple small files. 
- So, when there is a large amount of data in a single file, name node will occupy less space. Hence for getting optimized performance, HDFS supports large data sets instead of multiple small files.

117) Why a secondary namenode can't act as namenode?
Answer:
- It is not design to act as namenode.
- It keeps the recent fsimage of namenode.
- But it can work as namenode after fsimage is merged with latest edits and blocks mapping is after communication with data nodes.
- In case namenode fails suddenly and we are unable to access its edits, then latest updates over HDFS file structure will be lost as 
   Secondary namenode stores only the recent fsiamge but not the edits

118) How KMS works.?
Ans:
Hadoop Key Management Server (KMS) is a cryptographic key management server based on the Hadoop KeyProvider API. 
It provides a KeyProvider implementation client that interacts with the KMS using the HTTP REST API. 
Both the KMS and its client support HTTP SPNEGO Kerberos authentication and TLS/SSL-secured communication. 

119) What key trustee server and KMS? explain the difference?

Answer:
a) Key trustee server: Cloudera Navigator Key Trustee Server is an enterprise-grade virtual safe-deposit box that stores and manages cryptographic keys and other security artifacts
b) KMS: Hadoop Key Management Server (KMS) is a cryptographic key management server based on the Hadoop KeyProvider API.
Difference:
a) Cloudera strongly recommends using Key Trustee KMS in production environments to improve the security, durability, and scalability of your cryptographic key management
b) For parcel-based installations, no additional action is required to install or upgrade the KMS. For package-based installations, you must install additional packages.

120) Tell me about KMS how it works?
Answer:
Hadoop KMS is a cryptographic key management server based on Hadoop’s KeyProvider API.
It provides a client and a server components which communicate over HTTP using a REST API.
The client is a KeyProvider implementation interacts with the KMS using the KMS HTTP REST API.
KMS and its client have built-in security and they support HTTP SPNEGO Kerberos authentication and HTTPS secure transport.
KMS is a Java Jetty web-application.

121) What is the command to create keytab
 file?
Answer:
a) kadmin
bash-3.00$ kadmin -p admin@EXAMPLE.COM
kadmin: add_principal vemkd/cluster1@EXAMPLE.COM
b) ktutil
[root@test5~]#ktutil
ktutil: add_entry -password -p vemkd/cluster1@ibm.com -k 1 -e des3-cbc-sha1-kd 
Password for vemkd/cluster1@ibm.com:

122) How would user read his/her encrypted data which is stored in encryption zone?
Answer:
- To encrypt a new file, the HDFS client requests a new EDEK from the NameNode. The NameNode then asks the KMS to decrypt it with the encryption zone's EZ key. 
- This decryption results in a DEK, which is used to encrypt the file.
 
- To decrypt a file, the HDFS client provides the NameNode with the file's EDEK and the version number of the EZ key that was used to generate the EDEK.
- The NameNode requests the KMS to decrypt the file’s EDEK with the encryption zone's EZ key, which involves checking that the requesting client has permission to access that particular version of the EZ key.
- Assuming decryption of the EDEK is successful, the client then uses this DEK to decrypt the file.

123) What is kerberisation how it works?
Answer:
Kerberos is a network protocol that uses secret-key cryptography to authenticate client-server applications.
Works:
- When the user logins to machine. The principal, is sent to KDC server for login, and the KDC server will provide TGT in return
- Kdc server searches the principal name in the database, on finding the principal, a TGT is generated by the KDC, which will be encrypted by the users key, and send back to the user.
- When the user gets the TGT, the user decrypts the TGT with the help of KINIT.
- The TGT recieved by the client from the KDC server will be stored in the cache for use for the session duration.
- There will always be an expiration time set on the TGT offered by the KDC server, so that an expired TGT can never be used by an attacker.
- Now the client has got TGT in hand. If suppose the client needs to communicate with some service on that network, the client will ask the KDC server, for a ticket for that specific service with the help of TGT

124) What is KDC database ?
Ans. KDC – Key Distribution Centre is an Authentication server and a ticket granting server, 
when a client needs to access a resource on the server, the user credentials (password, Smart Card, biometrics) are presented to the Key Distribution Center (KDC) for authentication. 
If the user credentials are successfully verified in the Key Distribution Center (KDC), Key Distribution Center (KDC) issues a Ticket Granting Ticket (TGT) to the client. 
The Ticket Granting Ticket (TGT) is cached in the local machine for future use. The Ticket Granting Ticket (TGT) expires when the user disconnects or log off the network, or after it expires. 
The default expiry time is one day (86400 seconds)

125) What is keytab in Kerberos ?
Ans. A keytab is a file containing pairs of Kerberos principals and encrypted keys. 
Keytab files are commonly used to allow scripts to automatically authenticate various remote systems using Kerberos, without requiring human interaction.

126) Explain About Kerberos?
Answer:	
Kerberos is a secure method for authenticating a request for a service in a computer network. 
Kerberos was developed in the Athena Project at the Massachusetts Institute of Technology (MIT). 
The name is taken from Greek mythology; Kerberos was a three-headed dog who guarded the gates of Hades.

127) Kadmin used for?
Answer:
- kadmin provides for the maintenance of Kerberos principals, password policies, and service key tables (keytabs).
- The remote kadmin client uses Kerberos to authenticate to kadmind using the service principal.
- kadmin.local directly accesses the KDC database, it usually must be run directly on the master KDC with sufficient permissions to read the KDC database.
- If the KDC database uses the LDAP database module, kadmin.local can be run on any host which can access the LDAP server.

128) Security flow 
Ans:https://community.hortonworks.com/articles/102957/hadoop-security-concepts.html  ---Refer the diagram and description is there
Active Directory
============
Components
Authentication Server (AS)
Responsible for issuing Ticket Granting Tickets (TGT)
Ticket Granting Server (TGS)
Responsible for issuing service tickets
Key Distribution Center (KDC)
Talks with clients using KRB5 protocol
AS + TGS
LDAP Server
Contains user and group information and talks with its clients using the LDAP protocol.

Authentication
===========
Only a properly authenticated user (which can also be a service using another service) can communicate successfully with a kerberized Hadoop service. 
Missing the required authentication, in this case by proving the identity of both user and the service, any communication will fail. 
In a kerberized environment user authentication is provided via a ticket granting ticket (TGT).

Technical Authentication Flow
=======================
User requests TGT from AS. This is done automatically upon login or using the kinit command.
User receives TGT from AS.
User sends request to a kerberized service.
User gets service ticket from Ticket Granting Server. This is done automatically in the background when user sends a request to the service.
User sends service a request to the service using the service ticket.

129) What are the major differences between Hadoop 2 and Hadoop 3?
Answer:
Hadoop 2:
•	At least supports Java version 6
•	fault tolerance is achieved using replication
•	Limited shell scripting
•	YARN timeline service Introduced
•	Default port was conflicting with Linux port
•	Disk balancer is used
•	Doesn’t support Microsoft file system

Hadoop 3:
•	At least supports Java version 8
•	Fault tolerance is achieved using erasure(more efficient than replication since space sharing is enabled)
•	Enhance shell scripting with bug fixing
•	Improved YARN timeline service with more scalability and reliability
•	Optimized port range
•	Intra-DataNode balancing is added, which is invoked via the HDFS disk balancer CLI
•	Supports Microsoft Azure data lake
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------